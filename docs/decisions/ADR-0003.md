# ADR-0003: AI Workflow Kit - Standardized Development Tooling

> **Status:** Complete
> **Date:** 2025-12-04
> **Related Issue:** [](https://github.com/semops-ai/semops-core/issues/58)

---

## Executive Summary

Created a reusable CLI toolkit (`ai-workflow-kit`) to standardize AI-assisted development workflows across repositories. The toolkit combines Claude Code commands, Docker-based parallel experimentation, and optional spec-kit integration into a single installable package.

---

## Context

Working across multiple repositories (ike-publisher, ike-semantic-ops, etc.) requires consistent AI-assisted development tooling:

1. **Context priming** - Each repo needs a way to quickly onboard Claude to the codebase
2. **Parallel experimentation** - Testing different approaches (models, stacks, configurations) before committing to implementation
3. **Spec-driven development** - Structured requirements → planning → implementation workflow

Previously, these were set up manually in each repo, leading to:
- Inconsistent implementations across projects
- No easy way to update tooling across all repos
- Repeated setup work for new projects

Additionally, we identified two distinct parallelization strategies:
- **Git worktrees** - Multiple Claude Code subagents implementing the same feature differently (same stack, different code)
- **Docker containers** - Testing different stacks/configurations with isolated environments (different stack, same task)

Docker containers are better suited for our experimentation needs (comparing models, APIs, configurations).

---

## Decision

Create `ai-workflow-kit` as a versioned, installable Python CLI tool that:

1. **Installs globally** via `uv tool install`
2. **Initializes any repo** with `workflow init .`
3. **Provides standard templates**:
 - `/prime` - Context priming command (customizable key files)
 - `/prep-parallel-experiments` - Set up Docker experiment variants
 - `/execute-parallel-experiments` - Run and compare experiments
 - `experiments/template/` - Docker-based experiment framework
4. **Integrates spec-kit** (optional) for structured spec-driven development

### Hybrid Workflow

The toolkit supports a hybrid workflow combining spec-kit's structured thinking with empirical validation:

```
/speckit.specify → Define requirements
/speckit.plan → Architecture options
 ↓
/prep-parallel-experiments → Test different approaches in Docker
/execute-parallel-experiments → Compare results, pick winner
 ↓
/speckit.tasks → Generate tasks using validated approach
/speckit.implement → Build with confidence
```

### Repository

- **Location:** https://github.com/timjmitchell/ai-workflow-kit
- **Installation:** `uv tool install ai-workflow-kit --from git+https://github.com/timjmitchell/ai-workflow-kit.git`

---

## Consequences

**Positive:**
- Single command (`workflow init .`) sets up any repo with consistent tooling
- Versioned toolkit enables updates across all projects
- Docker-based experiments provide full isolation for testing different stacks
- Hybrid workflow combines structured planning with empirical validation
- No manual setup or copy-paste for new projects

**Negative:**
- Requires `uv` package manager (additional dependency)
- Initialized projects don't auto-update when toolkit updates (must re-run `workflow init . --force`)
- Docker required for experiment functionality

---

## Implementation Plan

### Phase 1: Core Toolkit ✓
- [x] Create `ai-workflow-kit` Python package
- [x] Implement `workflow init` CLI command
- [x] Create template commands (prime, prep-parallel, execute-parallel)
- [x] Create experiment template (Dockerfile, docker-compose, run.py)
- [x] Publish to GitHub

### Phase 2: Adoption
- [ ] Run `workflow init .` in ike-semantic-ops
- [ ] Run `workflow init .` in ike-publisher (already has manual setup)
- [ ] Customize `/prime` command in each repo with key files

### Phase 3: Iteration
- [ ] Refine experiment template based on usage
- [ ] Add additional command templates as patterns emerge
- [ ] Consider adding `workflow update` command for easier project updates

---

## Session Log

### 2025-12-04: Initial Implementation
**Status:** Completed
**Tracking Issue:** N/A (exploratory session)

**Completed:**
- Explored context engineering approaches (PRP vs spec-kit)
- Created ai-workflow-kit Python package with Typer CLI
- Implemented `workflow init`, `workflow list-templates`, `workflow version` commands
- Created templates: prime.md, prep-parallel-experiments.md, execute-parallel-experiments.md
- Created Docker experiment template
- Published to GitHub: https://github.com/timjmitchell/ai-workflow-kit
- Installed spec-kit in ike-publisher as integration test
- Installed uv/uvx package manager

**Key Insights:**
- spec-kit focuses on staged refinement (requirements → plan → implement)
- PRP focuses on one-pass implementation with dense context
- Docker experiments fill the gap for empirical validation of different approaches
- Hybrid workflow: use spec-kit for thinking, experiments for validation

**Next Session Should Start With:**
1. Run `workflow init .` in ike-semantic-ops
2. Customize prime.md with ike-semantic-ops key files
3. Test experiment workflow end-to-end

---

## References

- [ai-workflow-kit Repository](https://github.com/timjmitchell/ai-workflow-kit)
- [GitHub spec-kit](https://github.com/github/spec-kit)
- [uv Package Manager](https://docs.astral.sh/uv/)
- [Claude Code Documentation](https://docs.anthropic.com/en/docs/claude-code)

---

**End of Document**
